{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(4900) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(4901) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ply_path\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run and save results\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[43msafe_3d_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA modern office chair\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3D output saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36msafe_3d_generation\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"M2-optimized generation with output saving\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Stage 1: Text-to-image with MPS-optimized SD\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sd_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunwayml/stable-diffusion-v1-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m sd_pipe\u001b[38;5;241m.\u001b[39menable_attention_slicing()\n\u001b[1;32m     29\u001b[0m image \u001b[38;5;241m=\u001b[39m sd_pipe(\n\u001b[1;32m     30\u001b[0m     prompt,\n\u001b[1;32m     31\u001b[0m     num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m     32\u001b[0m     height\u001b[38;5;241m=\u001b[39mRESOLUTION,\n\u001b[1;32m     33\u001b[0m     width\u001b[38;5;241m=\u001b[39mRESOLUTION\n\u001b[1;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:454\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[0;32m--> 454\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    457\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    461\u001b[0m ):\n\u001b[1;32m    462\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:1031\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         )\n\u001b[0;32m-> 1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (7 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install diffusers accelerate transformers open3d --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = Path.cwd()\n",
    "RESOLUTION = 128  # Keep low for 8GB RAM\n",
    "STRIDE = 8        # Higher = fewer points\n",
    "\n",
    "def safe_3d_generation(prompt: str) -> Path:\n",
    "    \"\"\"M2-optimized generation with output saving\"\"\"\n",
    "    \n",
    "    # Stage 1: Text-to-image with MPS-optimized SD\n",
    "    sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    sd_pipe.enable_attention_slicing()\n",
    "    image = sd_pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=15,\n",
    "        height=RESOLUTION,\n",
    "        width=RESOLUTION\n",
    "    ).images[0]\n",
    "    \n",
    "    # Save intermediate image\n",
    "    img_path = OUTPUT_DIR / \"generated_image.png\"\n",
    "    image.save(img_path)\n",
    "    del sd_pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # Stage 2: Depth estimation\n",
    "    depth_pipe = pipeline(\n",
    "        \"depth-estimation\", \n",
    "        \"Intel/dpt-hybrid-midas\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    depth_map = depth_pipe(image)[\"depth\"]\n",
    "    del depth_pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # Stage 3: Point cloud generation\n",
    "    depth_array = np.array(depth_map.resize((RESOLUTION, RESOLUTION))) / 255.0\n",
    "    color_array = np.array(image.resize((RESOLUTION, RESOLUTION))) / 255.0\n",
    "\n",
    "    points, colors = [], []\n",
    "    for y in range(0, RESOLUTION, STRIDE):\n",
    "        for x in range(0, RESOLUTION, STRIDE):\n",
    "            points.append([x/RESOLUTION, y/RESOLUTION, depth_array[y, x]])\n",
    "            colors.append(color_array[y, x])\n",
    "\n",
    "    # Create and save point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.array(points))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.array(colors))\n",
    "    \n",
    "    ply_path = OUTPUT_DIR / \"output_3d.ply\"\n",
    "    o3d.io.write_point_cloud(str(ply_path), pcd)\n",
    "    \n",
    "    return ply_path\n",
    "\n",
    "# Run and save results\n",
    "output_file = safe_3d_generation(\"A modern office chair\")\n",
    "print(f\"3D output saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(6064) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6065) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/7c/29/7c2937659b52c19a0f95a3263e36d666c75eace5f0b2074383042784e0eee26a/40d8ea9159f3e875278dacc7879442d58c45850cf13c62f5e26681061c51829a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27diffusion_pytorch_model.fp16.safetensors%3B+filename%3D%22diffusion_pytorch_model.fp16.safetensors%22%3B&Expires=1738195270&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODE5NTI3MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yy8yOS83YzI5Mzc2NTliNTJjMTlhMGY5NWEzMjYzZTM2ZDY2NmM3NWVhY2U1ZjBiMjA3NDM4MzA0Mjc4NGUwZWVlMjZhLzQwZDhlYTkxNTlmM2U4NzUyNzhkYWNjNzg3OTQ0MmQ1OGM0NTg1MGNmMTNjNjJmNWUyNjY4MTA2MWM1MTgyOWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=iXGrIbkWJrXE8uY3Jp5ycDKZ-17iqHQgEotA4bAPIvba1Ocng7PZn%7EwFHz3uQKrDm-m0brs2E09bci1LR8meoVG05VPo%7ELhwEByF%7EarYL0%7E0d-EZ5Y3prAAglGqodNxPlF%7EXuhmd1gNLVRSFoRPqxPPmEXd3Ex1j%7EZzPLZTUPTk7LTfcHnPn618O44Opyu-nQ1TZT1dxYINKfQKEhYtP%7E4EPESecqhcG7FEAv%7EgL4FluoWfTWFJQzCavhdIq%7EIEBcB7mS-yg-Fs4XRK3wohDXFRs-Mkhsospofa7%7ELAGlR37j3%7EcDkU29tnLyWbA56W8YyQuYaut5EBQlqYXeJNxXg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/7c/29/7c2937659b52c19a0f95a3263e36d666c75eace5f0b2074383042784e0eee26a/d3df577f6e3799c8e1bd9b40e30133710e02e8e25d0ce48cdcc790e7dfe12d6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.fp16.safetensors%3B+filename%3D%22model.fp16.safetensors%22%3B&Expires=1738195269&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODE5NTI2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yy8yOS83YzI5Mzc2NTliNTJjMTlhMGY5NWEzMjYzZTM2ZDY2NmM3NWVhY2U1ZjBiMjA3NDM4MzA0Mjc4NGUwZWVlMjZhL2QzZGY1NzdmNmUzNzk5YzhlMWJkOWI0MGUzMDEzMzcxMGUwMmU4ZTI1ZDBjZTQ4Y2RjYzc5MGU3ZGZlMTJkNmQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=aqXnuit8BB7GF3IZOK1KrKzQei-oGYp3J8efDOxKPSKF6SN4FnqXlvpnFcrYheyVfIBYtcJ15gY78I79ypgfmKB1Psjixk%7ERyjayRfQaXIgAh9Iww0LdRYSM7yUO7alvrJLYFfGXULjkANth0hKtW-MHn7C0VPAWBdRw-dFcsjxa71mcp7LlnojxqeREMdCi8A98Wdzq8psx2XIjK4XJHPCJ5cKjAdlH1Vn5FfHrL-P3nhcz1AK1Y%7Eciqh73HnmLGH0OMk78Ab01Xf9JzJ1kzkRzPAl%7EVlh5j%7EHX4-qTT8gkhcWBZQfzkgnP4cIIStXSuzVneE93eNIAnZuLukrG8w__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install diffusers accelerate transformers open3d --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverSinglestepScheduler\n",
    "\n",
    "# Speed-optimized configuration\n",
    "DEVICE = \"mps\"\n",
    "RESOLUTION = 128  # 128px (don't go lower than 96)\n",
    "INFERENCE_STEPS = 12  # Minimum for decent quality\n",
    "STRIDE = 6 # Balance detail/speed\n",
    "OUTPUT_DIR = Path.cwd()\n",
    "\n",
    "def fast_3d_generation(prompt: str) -> Path:\n",
    "    \"\"\"Speed-optimized 3D generation pipeline\"\"\"\n",
    "    \n",
    "    # 1. Ultra-fast text-to-image (~45s)\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"segmind/SSD-1B\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.enable_attention_slicing()\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=INFERENCE_STEPS,\n",
    "        height=RESOLUTION,\n",
    "        width=RESOLUTION\n",
    "    ).images[0]\n",
    "    \n",
    "    # Save and clean up\n",
    "    img_path = OUTPUT_DIR / \"preview.png\"\n",
    "    image.save(img_path)\n",
    "    del pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # 2. Rapid depth estimation (~15s)\n",
    "    depth_pipe = pipeline(\n",
    "        \"depth-estimation\",\n",
    "        \"Intel/dpt-hybrid-midas\",\n",
    "        device=DEVICE,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    depth_map = depth_pipe(image)[\"depth\"]\n",
    "    del depth_pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # 3. Vectorized point cloud creation (~5s)\n",
    "    depth_array = np.array(depth_map.resize((RESOLUTION, RESOLUTION))) / 255.0\n",
    "    color_array = np.array(image.resize((RESOLUTION, RESOLUTION))) / 255.0\n",
    "    \n",
    "    # Vectorized grid sampling\n",
    "    x, y = np.meshgrid(np.arange(0, RESOLUTION, STRIDE), \n",
    "                      np.arange(0, RESOLUTION, STRIDE))\n",
    "    points = np.column_stack((\n",
    "        x.flatten()/RESOLUTION, \n",
    "        y.flatten()/RESOLUTION, \n",
    "        depth_array[y, x].flatten()\n",
    "    ))\n",
    "    colors = color_array[y, x].reshape(-1, 3)\n",
    "\n",
    "    # Save final output\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    ply_path = OUTPUT_DIR / \"output_3d.ply\"  # Fixed string termination\n",
    "    o3d.io.write_point_cloud(str(ply_path), pcd)\n",
    "    \n",
    "    return ply_path\n",
    "\n",
    "# Execution (Total ~1-2 mins)\n",
    "output_path = fast_3d_generation(\"A minimalist desk lamp\")\n",
    "print(f\"Generated in {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "os.environ['MallocStackLogging'] = '0'\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install diffusers accelerate transformers open3d --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Ultra-fast configuration\n",
    "DEVICE = \"mps\"\n",
    "RES = 128  # Minimum viable resolution\n",
    "STEPS = 8  # Fewer diffusion steps\n",
    "STRIDE = 8  # Sparse sampling\n",
    "\n",
    "def optimized_3d(prompt: str) -> Path:\n",
    "    \"\"\"Reliable 1-minute 3D generation\"\"\"\n",
    "    \n",
    "    # 1. Text-to-image with Apple-optimized model (~30s)\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"OFA-Sys/small-stable-diffusion-v0\",  # Verified working on MPS\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.enable_attention_slicing()\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=STEPS,\n",
    "        height=RES,\n",
    "        width=RES,\n",
    "        guidance_scale=3.0\n",
    "    ).images[0]\n",
    "    \n",
    "    # Save and cleanup\n",
    "    img_path = Path.cwd() / \"preview.png\"\n",
    "    image.save(img_path)\n",
    "    del pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # 2. Depth estimation (~15s)\n",
    "    depth_pipe = pipeline(\n",
    "        \"depth-estimation\",\n",
    "        \"Intel/dpt-swinv2-tiny-256\",  # Lightweight model\n",
    "        device=DEVICE\n",
    "    )\n",
    "    depth_map = depth_pipe(image)[\"depth\"]\n",
    "    del depth_pipe\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # 3. Efficient point cloud (~10s)\n",
    "    depth_array = np.array(depth_map.resize((RES, RES))) / 255.0\n",
    "    color_array = np.array(image.resize((RES, RES))) / 255.0\n",
    "    \n",
    "    # Grid sampling\n",
    "    y, x = np.indices((RES, RES))\n",
    "    mask = (x % STRIDE == 0) & (y % STRIDE == 0)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np.column_stack((\n",
    "        x[mask]/RES, y[mask]/RES, depth_array[mask]\n",
    "    )))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(color_array[mask])\n",
    "    \n",
    "    ply_path = Path.cwd() / \"output.ply\"\n",
    "    o3d.io.write_point_cloud(str(ply_path), pcd)\n",
    "    \n",
    "    return ply_path\n",
    "\n",
    "# Execute (Total ~55s-1.5m)\n",
    "output = optimized_3d(\"A simple chair\")\n",
    "print(f\"3D generated: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
